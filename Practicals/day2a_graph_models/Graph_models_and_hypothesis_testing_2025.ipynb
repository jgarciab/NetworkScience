{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWZO4GNOKp7c"
      },
      "source": [
        "# Graph Models and Hypothesis Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke4HKMVOKyu-"
      },
      "source": [
        "In this practical we will discover how use some simple models to generate synthetic networks and how we can use these models to test hypotheses about observed networks.\n",
        "\n",
        "Some of the libraries are difficult to install on some systems, so it is best to open this notebook in Colab [![Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jgarciab/NetworkScience/blob/main/Practicals/day2a_graph_models/Graph_models_and_hypothesis_testing_2025.ipynb)\n",
        "\n",
        "**You cannot save changes in this notebook, you need to save a copy to your Google Drive: `File` -> `Save a copy in Drive`.**\n",
        "\n",
        "First, we must install a useful library for analysing networks called [graph-tool](https://graph-tool.skewed.de/)..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!mamba install -q graph-tool"
      ],
      "metadata": {
        "id": "BLjE5NF5J4dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdenE7_WMFoj"
      },
      "source": [
        "Next we can import graph-tool and some other useful libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUjJQ-X9LkO2"
      },
      "outputs": [],
      "source": [
        "import graph_tool.all as gt\n",
        "from graph_tool import topology, inference, generation, stats, correlations\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import numpy as np\n",
        "from scipy.stats import binomtest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw1mcyRzB4wu"
      },
      "source": [
        "### PART I: Regular and random networks\n",
        "\n",
        "In the first part of this practical, we will consider some simple network models and the properties of the networks they generate.\n",
        "\n",
        "The graph-tool documentation for these network models is [here](https://graph-tool.skewed.de/static/doc/generation.html). **It will be useful to refer to the documentation to understand and use the code below**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slFYDLsCOvK-"
      },
      "source": [
        "#### Regular graphs and lattices\n",
        "Regular graphs are networks in which all nodes have the same degree ($k$).\n",
        "A lattice graph is a network in which all the nodes are arranged in a regular pattern, such as a ring or a square. Lattice graphs are often regular as all nodes have the same degree, with the exception of boundary nodes.\n",
        "\n",
        "The following code generates two examples of these types of graph: a square lattice ($k=4$, except boundary nodes) and a regular ring lattice ($k=2$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GokQrPQ9LufD"
      },
      "outputs": [],
      "source": [
        "# generate square lattice\n",
        "g = gt.lattice([10,10])\n",
        "# create layout\n",
        "pos = gt.sfdp_layout(g, cooling_step=0.95, epsilon=1e-2)\n",
        "# draw the network\n",
        "gt.graph_draw(g, pos=pos)\n",
        "\n",
        "# generate ring lattice\n",
        "g = gt.circular_graph(20, 2)\n",
        "# create layout\n",
        "pos = gt.sfdp_layout(g, cooling_step=0.95)\n",
        "# draw the network\n",
        "gt.graph_draw(g, pos=pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_IjbfVCUbfP"
      },
      "source": [
        "We know that regular graphs have constant degree. We can check the level of clustering (closed triangles) and average path length (diameter) using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBpvCYtmU1aW"
      },
      "outputs": [],
      "source": [
        "# print the number of nodes and edges in the network\n",
        "print('Number of nodes', g.num_vertices())\n",
        "print('Number of edges', g.num_edges())\n",
        "print('Clustering coefficient', gt.local_clustering(g).fa.mean())\n",
        "# Note that calculating the average path length exactly can be computationally\n",
        "# expensive for large networks, so here we use the pseudo diameter as an\n",
        "# approximation\n",
        "print('Average path length', gt.pseudo_diameter(g)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ_rE_WeTRW7"
      },
      "source": [
        "**Now try generating a ring lattice with 100 nodes and degree of 8 and calculate the average path length and clustering coefficient**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E34Wn49uReQ2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vEsZhocW86T"
      },
      "source": [
        "### Random (Erdos-Renyi) graphs\n",
        "Real-world networks tend to have more random connections than regular lattices. One of the simplest random graph models is the Erdos-Renyi model.\n",
        "\n",
        "In the Erdos-Renyi model every pair of nodes has the same probablity of having a link between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S--i7JNPT9LW"
      },
      "outputs": [],
      "source": [
        "g = gt.random_graph(100, lambda: np.random.poisson(10), directed=False)\n",
        "gt.graph_draw(g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_J_JZ2EZUnP"
      },
      "source": [
        "**Compute and display the clustering coefficient, path length and degree distribution for this random graph. What do you notice?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbjumIDUZFRk"
      },
      "outputs": [],
      "source": [
        "print('Number of nodes', # insert code here)\n",
        "print('Number of edges', # insert code here)\n",
        "print('Clustering coefficient', # insert code here)\n",
        "print('Average path length', # insert code here)\n",
        "\n",
        "# plot a histogram of the degrees\n",
        "degree_sequence = g.get_total_degrees(list(g.vertices()))\n",
        "_ = plt.hist(degree_sequence, bins=10)\n",
        "_ = plt.axvline(np.mean(degree_sequence), color='black')\n",
        "_ = plt.title('Mean Degree = {:.2f}'.format(np.mean(degree_sequence)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAqWpdk6B4wx"
      },
      "source": [
        "**Generate another random graph with 1000 nodes with the same expected degree and plot the degree distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-Vasr_PB4wx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcGd_t_8B4wx"
      },
      "source": [
        "We use the binomial distribution to determine the probability of observing a node with a degree of 15 or more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3rx2AUCB4wy"
      },
      "outputs": [],
      "source": [
        "# number of nodes\n",
        "n = 1000\n",
        "# mean degree\n",
        "c = 10\n",
        "# probability of a connection\n",
        "p = c/(n-1)\n",
        "# probability of a node having at least 15 connections\n",
        "test = binomtest(15, n-1, p, alternative='greater')\n",
        "test.pvalue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xP9Uk-wB4wy"
      },
      "source": [
        "**How many nodes are there in the network at least 15 connections?**\n",
        "\n",
        "**How many nodes would you expect to have a degree of at least 15?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfV5NMDLB4wy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek8eA_UJB4wy"
      },
      "source": [
        "The code below loads a network in face-to-face interactions between high school students during the first hour of the school day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99kp-jtcB4wy"
      },
      "outputs": [],
      "source": [
        "!wget https://networks.skewed.de/net/sp_high_school/files/proximity.gt.zst\n",
        "g_proximity = gt.load_graph(\"proximity.gt.zst\", )\n",
        "\n",
        "# create a graph view of the interactions in the first hour of the dataset\n",
        "g_prox_hour1 = gt.GraphView(g_proximity, efilt = lambda e: g_proximity.ep.time[e] <= 1385982020 + 3600)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrBo5ZorB4wy"
      },
      "source": [
        "**Considering the degree distribution, can you perform a statistical test to show that the interaction patterns are not modelled well with a ER random graph?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFt-L8uvB4wy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFYOlLfDB4wy"
      },
      "source": [
        "**Can you perform the same sort of tests using the clustering coefficient?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4XMA5i9B4wy"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WqHN2PdMoC8"
      },
      "source": [
        "### PART II: Small worlds and heavy tails\n",
        "\n",
        "In the first part of this practical, we will consider some simple network models and the properties of the networks they generate.\n",
        "\n",
        "The graph-tool documentation for these network models is [here](https://graph-tool.skewed.de/static/doc/generation.html). **It will be useful to refer to the documentation to understand and use the code below**\n",
        "\n",
        "Recall from the lecture that there are a number of properties that real-world networks often exhibit:\n",
        "\n",
        "1.   High clustering\n",
        "2.   Short path lengths\n",
        "3.   Heavy-tailed degree distributions\n",
        "\n",
        "We will see that the following network models capture these properties to varying extents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVF2GPDknrOR"
      },
      "source": [
        "### Small-world networks\n",
        "\n",
        "Small-world networks are networks that share properties of regular networks and random networks. Small-world networks are created by randomising a proportion of the links in a regular network, while preserving part of the regular structure.\n",
        "\n",
        "![image.png](https://drive.google.com/uc?id=1P8ewNjJuGlnIvZp1RKF2WCQwAFaZ8AL5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmLDLvlHkg6b"
      },
      "outputs": [],
      "source": [
        "n_networks = 14\n",
        "rewire_probs = np.logspace(-4, 1, n_networks)\n",
        "n_nodes = 1000\n",
        "n_edges = 5000\n",
        "g = gt.circular_graph(n_nodes, n_edges/n_nodes)\n",
        "\n",
        "C0 = gt.global_clustering(g)[0]\n",
        "L0 = gt.pseudo_diameter(g)[0]\n",
        "\n",
        "clusts = np.empty(n_networks)\n",
        "lengths = np.empty(n_networks)\n",
        "\n",
        "for i in range(n_networks):\n",
        "  # generate ring lattice\n",
        "  g = gt.circular_graph(n_nodes, n_edges/n_nodes)\n",
        "  p = rewire_probs[i]\n",
        "  generation.random_rewire(g, n_iter=np.round(p*n_edges), edge_sweep=False)\n",
        "  clusts[i] = gt.global_clustering(g)[0]/C0\n",
        "  lengths[i] = gt.pseudo_diameter(g)[0]/L0\n",
        "\n",
        "plt.semilogx(rewire_probs, clusts, 'o', label='clustering')\n",
        "plt.semilogx(rewire_probs, lengths, 'o', label='diameter')\n",
        "plt.legend()\n",
        "_ = plt.xlabel('Rewire probability (p)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTCY7Nop4JNX"
      },
      "source": [
        "**What range of $p$ produces \"realistic\" networks?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yKHnddY4Ses"
      },
      "source": [
        "### The Price Model (preferential attachment)\n",
        "\n",
        "The models so far capture some important properties commonly observed in real-world networks, however they all create networks with a relatively homogenous degree distribution.\n",
        "\n",
        "The Price model (or Barabási-Albert model, if undirected) creates networks based on a mechanism known as preferential attachment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gas30Al0lmfp"
      },
      "outputs": [],
      "source": [
        "g = gt.price_network(5000)\n",
        "\n",
        "gt.graph_draw(g, pos=gt.sfdp_layout(g, cooling_step=0.99),\n",
        "              vertex_fill_color=g.vertex_index, vertex_size=2,\n",
        "              vcmap=mpl.cm.plasma,\n",
        "              edge_pen_width=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2TUsVPr-uAZ"
      },
      "source": [
        "Above the colours indicate the order in which the nodes were added to the network.\n",
        "\n",
        "We can now plot the degree distribution on a log-log axes..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-y_dGaYms4R"
      },
      "outputs": [],
      "source": [
        "# Let's plot the degree distribution\n",
        "in_hist = gt.vertex_hist(g, \"total\")\n",
        "\n",
        "y = in_hist[0]\n",
        "err = np.sqrt(in_hist[0])\n",
        "err[err >= y] = y[err >= y] - 1e-2\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "\n",
        "plt.errorbar(in_hist[1][:-1], in_hist[0], fmt=\"o\", yerr=0, label=\"in\")\n",
        "\n",
        "plt.gca().set_yscale(\"log\")\n",
        "plt.gca().set_xscale(\"log\")\n",
        "plt.gca().set_ylim(1e-1, 1e5)\n",
        "plt.gca().set_xlim(0.8, 1e3)\n",
        "plt.subplots_adjust(left=0.2, bottom=0.2)\n",
        "plt.xlabel(\"$k_{in}$\")\n",
        "plt.ylabel(\"$NP(k_{in})$\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGPusExRAHMG"
      },
      "source": [
        "### The configuration model\n",
        "\n",
        "The configuration model is an extention of the random graph model in which an arbitrary degree sequence can be specified.\n",
        "\n",
        "We can generate a configuration model network using the `random_graph` function in graph-tool, by specifying a function that generates a heavy-tailed distribution to set the node degrees.\n",
        "\n",
        "However, an alternative is to use a network that already has a heavy-tailed degree sequence and rewire the edges such that the degrees are preserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iObR5Uv_Z2c"
      },
      "outputs": [],
      "source": [
        "generation.random_rewire(g, model='configuration')\n",
        "gt.graph_draw(g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79AffNbkB4w0"
      },
      "source": [
        "### The Friendship paradox\n",
        "\n",
        "**Test the proxmity network from earlier to determine if the friendship paradox holds.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UNpl3lOB4w0"
      },
      "outputs": [],
      "source": [
        "# load the data and remove parallel edges\n",
        "g_proximity = gt.load_graph(\"proximity.gt.zst\", )\n",
        "gt.remove_parallel_edges(g_proximity)\n",
        "\n",
        "# create an adjacency matrix\n",
        "A = gt.adjacency(g_proximity)\n",
        "# calculate a vector of degrees\n",
        "k = np.sum(A, axis=1)\n",
        "# calculate the mean of friends degrees\n",
        "friend_degrees = # insert code here\n",
        "# compare with the actual degrees\n",
        "popular_nodes =  # insert code here\n",
        "print('The proportion of nodes that have more friends than their average friend is', popular_nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRAz_0NVUNGQ"
      },
      "source": [
        "### PART III: The BESTest\n",
        "\n",
        "The following gives an example on how to run the BESTest on the Lazega lawyers networks. Try out the code for yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it_iBAzMNd7D"
      },
      "outputs": [],
      "source": [
        "# function to get entropy of a random partition\n",
        "def random_entropy(g, n_groups):\n",
        "\n",
        "  n = g.num_vertices()\n",
        "  b = np.random.randint(n_groups, size=n)\n",
        "  vprop_b = g.new_vertex_property(\"int\")\n",
        "  for i in range(n):\n",
        "    v = g.vertex(i)\n",
        "    vprop_b[v] = b[i]\n",
        "\n",
        "  state = gt.BlockState(g, b=vprop_b)\n",
        "  return state.entropy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdErV-2OUo9a"
      },
      "outputs": [],
      "source": [
        "!wget 'https://github.com/jgarciab/NetworkScience/raw/main/Data/law_firm.gt.zst'\n",
        "g = gt.load_graph('law_firm.gt.zst')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZgOYHqpVAdS"
      },
      "outputs": [],
      "source": [
        "# create a graph with layer 1\n",
        "u = gt.GraphView(g, efilt=lambda e: g.ep.layer[e] == 1)\n",
        "g1 = gt.Graph(u, prune=True)\n",
        "print(list(g1.vp.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPEE7f7jVM_h"
      },
      "outputs": [],
      "source": [
        "attr_name = 'nodeOffice'\n",
        "state = gt.BlockState(g1, b=g1.vp[attr_name])\n",
        "test_value = state.entropy()\n",
        "print('observed test statistic (entropy)', test_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rK4tFhHVh5A"
      },
      "outputs": [],
      "source": [
        "n_samples = 1000\n",
        "n_groups = len(set(g1.vp[attr_name]))\n",
        "null_entropy = np.empty(n_samples)\n",
        "for i in range(n_samples):\n",
        "  null_entropy[i] = random_entropy(g1, n_groups)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM9syxLbWo8u"
      },
      "outputs": [],
      "source": [
        "plt.hist(null_entropy, bins=50)\n",
        "_ = plt.axvline(test_value, color='black')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3xWfsakB4w1"
      },
      "source": [
        "**Can you also run the BESTest on the proximity network?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYl3pIFRW-sD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIOBqA18B4w2"
      },
      "source": [
        "### Part IV: Reconstructing networks from noisy data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlEXI8vMB4w2"
      },
      "outputs": [],
      "source": [
        "# load the data and remove parallel edges\n",
        "g_proximity = gt.load_graph(\"proximity.gt.zst\", )\n",
        "gt.remove_parallel_edges(g_proximity)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8D7rpYNB4w2"
      },
      "source": [
        "**Try rewiring the edges to introduce some \"noise\" into the network structure. What happens to the clustering coefficient, path length and assortativity?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbKsWPE6B4w3"
      },
      "outputs": [],
      "source": [
        "## Complete the code to calculate the clustering coefficient, average path length, and assortativity of the network as noise is added.\n",
        "\n",
        "# make a copy of the graph to add noise\n",
        "g_proximity_with_noise = g_proximity.copy()\n",
        "\n",
        "# calculate the clustering coefficient\n",
        "clustering = gt.local_clustering(g_proximity_with_noise).fa.mean()\n",
        "# calculate the average path length\n",
        "diameter = gt.pseudo_diameter(g_proximity_with_noise)[0]\n",
        "# calculate the assortativity\n",
        "assortativity = correlations.assortativity(g_proximity_with_noise, 'total')[0]\n",
        "\n",
        "# add noise to the graph by rewiring the edges\n",
        "generation.random_rewire(g_proximity_with_noise, n_iter=100, edge_sweep=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWv4lmaXB4w3"
      },
      "source": [
        "Now we will attempt to reconstruct the network from a noisy version.\n",
        "\n",
        "First we generate a noisy version by rewiring 100 edges..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPQJJnm6B4w3"
      },
      "outputs": [],
      "source": [
        "# make a copy of the graph to add noise\n",
        "g_proximity_with_noise = g_proximity.copy()\n",
        "generation.random_rewire(g_proximity_with_noise, n_iter=100, edge_sweep=False)\n",
        "\n",
        "# calculate the similarity between the original and the noisy graph\n",
        "similarity = topology.similarity(g_proximity, g_proximity_with_noise)\n",
        "print('Similarity', similarity)\n",
        "\n",
        "# calculate clustering coefficient of original and noisy graph\n",
        "clustering = gt.local_clustering(g_proximity).fa.mean()\n",
        "clustering_with_noise = gt.local_clustering(g_proximity_with_noise).fa.mean()\n",
        "print('Clustering coefficient', clustering)\n",
        "print('Clustering coefficient with noise', clustering_with_noise)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhl51O8SB4w3"
      },
      "source": [
        "Now we attempt the reconstruction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6jNXXXjB4w4"
      },
      "outputs": [],
      "source": [
        "n = g_proximity_with_noise.new_ep(\"int\", 1)   # number of measurements\n",
        "x = g_proximity_with_noise.new_ep(\"int\", 1)   # number of observations\n",
        "\n",
        "# We inititialize MeasuredBlockState, assuming that each non-edge has\n",
        "# been measured only once (n_default=1) and not observed (x_default=0)\n",
        "\n",
        "state = gt.MeasuredBlockState(g_proximity_with_noise, n=n, n_default=1, x=x, x_default=0)\n",
        "\n",
        "# We will first equilibrate the Markov chain\n",
        "gt.mcmc_equilibrate(state, wait=100, mcmc_args=dict(niter=10))\n",
        "\n",
        "# Now we collect the marginals for exactly 100,000 sweeps, at\n",
        "# intervals of 10 sweeps:\n",
        "\n",
        "u = None              # marginal posterior edge probabilities\n",
        "bs = []               # partitions\n",
        "cs = []               # average local clustering coefficient\n",
        "\n",
        "def collect_marginals(s):\n",
        "   global u, bs, cs\n",
        "   u = s.collect_marginal(u)\n",
        "   bstate = s.get_block_state()\n",
        "   bs.append(bstate.levels[0].b.a.copy())\n",
        "   cs.append(gt.local_clustering(s.get_graph()).fa.mean())\n",
        "\n",
        "gt.mcmc_equilibrate(state, force_niter=10000, mcmc_args=dict(niter=10),\n",
        "                    callback=collect_marginals)\n",
        "\n",
        "eprob = u.ep.eprob\n",
        "print(\"Estimated average local clustering: %g ± %g\" % (np.mean(cs), np.std(cs)))\n",
        "\n",
        "g_inferred = gt.GraphView(u, efilt = lambda e: u.ep.eprob[e] >= 0.5)\n",
        "similarity = topology.similarity(g_proximity, g_inferred)\n",
        "print('Similarity', similarity)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00QG4cHxB4w4"
      },
      "source": [
        "We can also attempt to do the reconstruction using multiple noisy measurements..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXEAwlXwB4w4"
      },
      "outputs": [],
      "source": [
        "# create another graph with noise\n",
        "g_proximity_with_noise1 = g_proximity.copy()\n",
        "generation.random_rewire(g_proximity_with_noise1, n_iter=1000, edge_sweep=False)\n",
        "\n",
        "x1 = g_proximity_with_noise1.new_ep(\"int\", 1)\n",
        "\n",
        "for edge in g_proximity_with_noise1.edges():\n",
        "    try:\n",
        "        x[(edge.source(), edge.target())] += 1\n",
        "        n[(edge.source(), edge.target())] += 1\n",
        "\n",
        "    except IndexError:\n",
        "        g_proximity_with_noise.add_edge(edge.source(), edge.target())\n",
        "        x[(edge.source(), edge.target())] = 1\n",
        "\n",
        "n_obs += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDIC8qDAB4w4"
      },
      "source": [
        "Now we combine the measurements to perform the reconstruction..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4diZqKh7B4w4"
      },
      "outputs": [],
      "source": [
        "n = g_proximity_with_noise.new_ep(\"int\", n_obs)   # number of measurements\n",
        "\n",
        "state = gt.MeasuredBlockState(g_proximity_with_noise, n=n, n_default=n_obs, x=x, x_default=0)\n",
        "\n",
        "# We will first equilibrate the Markov chain\n",
        "gt.mcmc_equilibrate(state, wait=100, mcmc_args=dict(niter=10))\n",
        "\n",
        "# Now we collect the marginals for exactly 100,000 sweeps, at\n",
        "# intervals of 10 sweeps:\n",
        "\n",
        "u = None              # marginal posterior edge probabilities\n",
        "bs = []               # partitions\n",
        "cs = []               # average local clustering coefficient\n",
        "\n",
        "def collect_marginals(s):\n",
        "   global u, bs, cs\n",
        "   u = s.collect_marginal(u)\n",
        "   bstate = s.get_block_state()\n",
        "   bs.append(bstate.levels[0].b.a.copy())\n",
        "   cs.append(gt.local_clustering(s.get_graph()).fa.mean())\n",
        "\n",
        "gt.mcmc_equilibrate(state, force_niter=10000, mcmc_args=dict(niter=10),\n",
        "                    callback=collect_marginals)\n",
        "\n",
        "eprob = u.ep.eprob\n",
        "print(\"Estimated average local clustering: %g ± %g\" % (np.mean(cs), np.std(cs)))\n",
        "\n",
        "g_inferred = gt.GraphView(u, efilt = lambda e: u.ep.eprob[e] >= 0.5)\n",
        "similarity = topology.similarity(g_proximity, g_inferred)\n",
        "print('Similarity', similarity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnxzOPANB4w4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}